<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html;charset=iso-8859-1">
<title>Dr.L.: Dimensionality Reduction Library (Dr.L.)</title>
<link href="doxygen.css" rel="stylesheet" type="text/css">
</head><body>
<!-- Generated by Doxygen 1.3.9.1 -->
<div class="qindex"><a class="qindexHL" href="index.html">Main&nbsp;Page</a> | <a class="qindex" href="hierarchy.html">Class&nbsp;Hierarchy</a> | <a class="qindex" href="annotated.html">Class&nbsp;List</a> | <a class="qindex" href="files.html">File&nbsp;List</a> | <a class="qindex" href="functions.html">Class&nbsp;Members</a> | <a class="qindex" href="globals.html">File&nbsp;Members</a></div>
<h1>Dimensionality Reduction Library (Dr.L.) </h1>
<p>
<h3 align="center">0.2 </h3><h2><a class="anchor" name="intro">
Introduction</a></h2>
Dr. L. is a C++ library for dimensionality reduction. In the context of this library, dimensionality reduction is considered to consist of: <ol>
</ol>
<ol type=1>
<li>Estimation of the intrinsic dimensionality using sampled data</li><li>Finding maps that reduce the dimensionality of data (forward map) or increase the dimensionality of data (reverse map). The "find" is performed by optimizing some metric based on the sampled data. In the ideal case, applying the forward map followed by the reverse map should regenerate the original data points.</li><li>Mapping arbitrary coordinates to high and low dimensionalities </li></ol>
<p>
The library is intended to provide a consistent interface to multiple dimensionality reduction algorithms with an efficient C++ interface that runs efficiently on multicore architectures. A few routines have been optimized with an option for GPU acceleration or distributed computation.<p>
Currently, the library offers intrinsic dimensionality estimation using: <ol>
</ol>
<ol type=1>
<li>point-PCA</li><li>reconstruction error</li><li>residual variance. </li></ol>
<p>
The following dimensionality reduction methods have been implemented. <ol>
</ol>
<ol type=1>
<li>Principal Component Analysis</li><li>Multidimensional Scaling</li><li>Locally Linear Embedding</li><li>IsoMap</li><li>Autoencoder Neural Networks </li></ol>
<p>
An executable is also supplied that can be built to allow for command-line access to the library routines. A description of an application of the library for molecular structure analysis has been published <a class="el" href="index.html#ref">[Brown, 2009]</a>.<p>
<br>
 <br>
 <h2><a class="anchor" name="dimred">
Dimensionality Reduction</a></h2>
In general, dimensionality reduction algorithms provide a method for taking a set of samples <img class="formulaInl" alt="$\{\mathbf{x}_1, \dots, \mathbf{x}_n \} \subset \mathbb{R}^D$" src="form_0.png"> and calculating a corresponding low-dimensional representation <img class="formulaInl" alt="$\{\mathbf{y}_1, \dots, \mathbf{y}_n \} \subset \mathbb{R}^d$" src="form_1.png">. Because dimensionality reduction is often used for visualization, some algorithms do not generate an explicit map from the high dimensional coordinates to the low dimensional representation. For many applications, however, it is desirable to have an explicit forward map, <img class="formulaInl" alt="$\Phi(\mathbf{x}) : \mathbb{R}^D \rightarrow \mathbb{R}^d$" src="form_2.png">, that gives the low-dimensional representation of an arbitrary point <img class="formulaInl" alt="$\mathbf{x}$" src="form_3.png"> and an explicit reverse map <img class="formulaInl" alt="$\phi(\mathbf{y}) : \mathbb{R}^d \rightarrow \mathbb{R}^D$" src="form_4.png"> that gives the high-dimensional represenation of an arbitrary point <img class="formulaInl" alt="$\mathbf{y}$" src="form_5.png">. This allows for mapping new samples that were not available at the time of the initial reduction and also provides a common metric for comparison of algorithms. Therefore, for the purposes of this work, we consider dimensionality reduction as the problem of generating <img class="formulaInl" alt="$\Phi$" src="form_6.png"> and <img class="formulaInl" alt="$\phi$" src="form_7.png"> from a training set of <img class="formulaInl" alt="$n$" src="form_8.png"> samples, <img class="formulaInl" alt="$X_{D \times n} = (\mathbf{x}_1, \dots, \mathbf{x}_n )$" src="form_9.png">. Because some methods do not generate explicit maps, we describe an approach for generating maps from a dimensionality reduction below.<p>
The performance of each algorithm can be evaluated using the reconstruction error. Ideally, a forward map of an arbitrary point followed by a reverse map will give the same point back. Typically, the reconstruction error is given by <img class="formulaInl" alt="$\|\mathbf{x}-\phi(\Phi(\mathbf{x}))\|$" src="form_10.png">.<p>
<br>
 <br>
 <h2><a class="anchor" name="ns">
Namespace</a></h2>
Although hidden from doxygen, the Dr. L. library resides in the <em>yala</em> namespace. You can access Dr. L. commands in your code with the following lines:<p>
<div class="fragment"><pre class="fragment"><span class="preprocessor">#include "dimred/ya_dimred.h"</span>
<span class="keyword">using</span> <span class="keyword">namespace </span>yala;
</pre></div><p>
<br>
 <br>
 <h2><a class="anchor" name="vecmat">
Matrix and Vector Storage in Dr. L.</a></h2>
Dr. L. uses a custom linear algebra library for computations. To incorporate the library with your code, wrappers can be used for column-major or row-major matrices stored in traditional continuous memory arrays, STL vectors, or standard library valarrays. The wrapper used will depend on whether each datapoint is stored in a row or a column and whether or not the matrix is column major.<p>
<div class="fragment"><pre class="fragment"><span class="preprocessor">#include "dimred/ya_dimred.h"</span>
<span class="keyword">using</span> <span class="keyword">namespace </span>yala;

<span class="keywordtype">int</span> main(<span class="keywordtype">int</span> argc, <span class="keywordtype">char</span> **argv) {
  <span class="keywordtype">int</span> rows=4;
  <span class="keywordtype">int</span> cols=3;
  <span class="keywordtype">int</span> numel=rows*cols;
  <span class="keywordtype">double</span> input[numel];
  
  <span class="comment">// Fill the matrix....</span>
  
  <span class="comment">// If each row is a datapoint (each column is a dimension)</span>
  <span class="comment">// --If the matrix is column major</span>
  YA_WRAP(<span class="keywordtype">double</span>) input_w(input,rows,cols);
  <span class="comment">// --If the matrix is row major</span>
  YA_WRAP_RM(<span class="keywordtype">double</span>) input_w(input,rows,cols);
  
  <span class="comment">// If each column is a datapoint (each row is a dimension)</span>
  <span class="comment">// --If the matrix is column major</span>
  YA_WRAP_RM(<span class="keywordtype">double</span>) input_w(input,cols,rows);
  <span class="comment">// --If the matrix is row major</span>
  YA_WRAP(<span class="keywordtype">double</span>) input_w(input,cols,rows);

  <span class="comment">// Use input_w with Dr. L. routines...</span>
  <span class="comment">// Same procedure can be used for output matrices.</span>
  
  <span class="comment">// Use input as usual...</span>
  
  return 0;
}
</pre></div><p>
For single precision, double is replaced with float in the wrappers above, etc. For STL vectors:<p>
<div class="fragment"><pre class="fragment">  std::vector&lt;double&gt; input;
  
  ...
  
  
  YA_VWRAP(<span class="keywordtype">double</span>) input_w(input.begin(),rows,cols);
  
  <span class="comment">// or</span>
  
  YA_VWRAP_RM(<span class="keywordtype">double</span>) input_w(input.begin(),rows,cols);
</pre></div><p>
For valarray:<p>
<div class="fragment"><pre class="fragment">  std::valarray&lt;double&gt; input;
  
  ...
  
  YA_WRAP(<span class="keywordtype">double</span>) input_w(&amp;input[0],rows,cols);
  
  <span class="comment">// or</span>
  
  YA_WRAP_RM(<span class="keywordtype">double</span>) input_w(&amp;input[0],rows,cols);
</pre></div><p>
<br>
 <br>
 <h2><a class="anchor" name="general">
General Dimensionality Reduction Routines</a></h2>
Several general dimensionality reduction routines are available through the pure virtual class <a class="el" href="class_y_a_dim_reduce.html">YADimReduce</a>. The base class can also be used for run-time selection of dimensionality routines:<p>
<div class="fragment"><pre class="fragment">  <a class="code" href="class_y_a_dim_reduce.html">YADimReduce&lt;double&gt;</a> *redmet=NULL;
  
  <span class="keywordflow">if</span> (method_str==<span class="stringliteral">"PCA"</span>)
    redmet=<span class="keyword">new</span> <a class="code" href="class_y_a_p_c_a_reduce.html">YAPCAReduce&lt;double&gt;</a>;
  <span class="keywordflow">else</span> <span class="keywordflow">if</span> (method_str==<span class="stringliteral">"MDS"</span>)
    redmet=<span class="keyword">new</span> <a class="code" href="class_y_a_m_d_s_reduce.html">YAMDSReduce&lt;double&gt;</a>;
  <span class="keywordflow">else</span> <span class="keywordflow">if</span> (method_str==<span class="stringliteral">"LLE"</span>)
    redmet=<span class="keyword">new</span> <a class="code" href="class_y_a_l_l_e_reduce.html">YALLEReduce&lt;double&gt;</a>;
  <span class="keywordflow">else</span> <span class="keywordflow">if</span> (method_str==<span class="stringliteral">"ISOMAP"</span>)
    redmet=<span class="keyword">new</span> <a class="code" href="class_y_a_iso_reduce.html">YAIsoReduce&lt;double&gt;</a>;
  <span class="keywordflow">else</span> <span class="keywordflow">if</span> (method_str==<span class="stringliteral">"AE"</span>)
    redmet=<span class="keyword">new</span> <a class="code" href="class_y_a_a_t_e_reduce.html">YAATEReduce&lt;double&gt;</a>;
</pre></div><p>
The following sample code describes some of the routines available through the base class:<p>
<div class="fragment"><pre class="fragment">  <span class="comment">// Store the high dimensionality and low dimensionality for maps somewhere</span>
  <span class="keywordtype">int</span> high_dim=5;
  <span class="keywordtype">int</span> low_dim=3;
  
  <span class="comment">// For methods that utilize eigen decomposition, options are set through</span>
  <span class="comment">// the EigenOptions class.</span>
  EigenOptions eigopts;
  
  <span class="comment">// Set the degree of output to the console</span>
  <span class="comment">// -- 0 is not output, 1 outputs a progress meter, 2 also outputs nbor stats</span>
  redmet-&gt;<a class="code" href="class_y_a_dim_reduce.html#a16">verbose</a>(2);

  <span class="comment">// The find routine uses a set of input samples to obtain forward and reverse</span>
  <span class="comment">// maps. Additionally, this routine will store the low-dimensional </span>
  <span class="comment">// representation for a set of samples in an output matrix. </span>
  redmet-&gt;<a class="code" href="class_y_a_dim_reduce.html#a5">find_t</a>(input, output, low_dim, eigopts);
  
  <span class="comment">// Map in the forward direction to reduce the dimensionality of a matrix</span>
  redmet-&gt;<a class="code" href="class_y_a_dim_reduce.html#a6">forward_t</a>(input_mat, output_mat);
  
  <span class="comment">// Map in the reverse direction to increase the dimensionality of a matrix</span>
  redmet-&gt;<a class="code" href="class_y_a_dim_reduce.html#a7">reverse_t</a>(input_mat, output_mat);
  
  <span class="comment">// Calculate RMSD representing the reconstruction error from the input matrix</span>
  <span class="keywordtype">double</span> err = redmet-&gt;<a class="code" href="class_y_a_dim_reduce.html#a11">reconstruct_error</a>(input);  
    
  <span class="comment">// Save dimensionality reduction maps to the given file name</span>
  <span class="keywordtype">int</span> error_flag=redmet-&gt;<a class="code" href="class_y_a_dim_reduce.html#a12">save_map</a>(<span class="stringliteral">"example_maps.dat"</span>);
  
  <span class="comment">// Load dimensionality reduction maps from the given file name</span>
  <span class="keywordtype">int</span> error_flag=redmet-&gt;<a class="code" href="class_y_a_dim_reduce.html#a14">load_map</a>(<span class="stringliteral">"example_maps.dat"</span>);
</pre></div><p>
For a list of additional routines, click on <a class="el" href="class_y_a_dim_reduce.html">YADimReduce</a>.<p>
<br>
 <br>
 <h2><a class="anchor" name="nbors">
Neighbor Reconstruction Mapping</a></h2>
LLE and Isomap produce a low-dimensional embedding <img class="formulaInl" alt="$Y_{d \times n} = \{\mathbf{y}_1, \dots, \mathbf{y}_n \} \subset \mathbb{R}^d$" src="form_11.png"> from the samples in <img class="formulaInl" alt="$X$" src="form_12.png"> without generating an explicit map. Here, we have considered dimensionality reduction as a problem of finding the maps <img class="formulaInl" alt="$\Phi$" src="form_6.png"> and <img class="formulaInl" alt="$\phi$" src="form_7.png"> from training data. For LLE and Isomap, we accomplish this with the maps <img class="formulaInl" alt="$\Phi_{NRM}(X,Y,\mathbf{x})$" src="form_13.png"> and <img class="formulaInl" alt="$\phi_{NRM}(X,Y,\mathbf{y})$" src="form_14.png"> that allow for dimensionality reduction to be performed on future samples based on the initial embedding of training data. A natural choice for these maps is some method that retains the positioning of a sample relative to its neighbors in the training set. Because LLE and Isomap assume that a sample and its neighbors are locally linear, we can perform the mapping using a linear combination of a sample's <img class="formulaInl" alt="$k$" src="form_15.png"> neighbors:<p>
<p class="formulaDsp">
<img class="formulaDsp" alt="\[ \Phi_{NRM}(X,Y,\mathbf{x})=\displaystyle\sum_{i=1}^k w_i\mathbf{y}_i \]" src="form_16.png">
<p>
 and <p class="formulaDsp">
<img class="formulaDsp" alt="\[ \phi_{NRM}(X,Y,\mathbf{y})=\displaystyle\sum_{i=1}^k w_i\mathbf{x}_i. \]" src="form_17.png">
<p>
<p>
That is, the training set neighbors for an arbtrary point <img class="formulaInl" alt="$\mathbf{x}$" src="form_3.png"> or <img class="formulaInl" alt="$\mathbf{y}$" src="form_5.png"> can be identified in the input dimensionality and used to determine the sample mapping based on their positions (<img class="formulaInl" alt="$\mathbf{x}_i$" src="form_18.png"> or <img class="formulaInl" alt="$\mathbf{y}_i$" src="form_19.png">) in the desired dimensionality. The question is how to choose the weights <img class="formulaInl" alt="$w_i$" src="form_20.png">. The equations bear a strong resemblence to the reconstruction approach used in LLE and it has been suggested that this same approach can used to map new samples <a class="el" href="index.html#ref">[Saul, 2003]</a>. In this case, <img class="formulaInl" alt="$w_i$" src="form_20.png"> are determined in a least-squares optimization with a closed form solution. There are issues in implementing this approach, however. For the case when the number of neighbors <img class="formulaInl" alt="$k$" src="form_15.png"> is greater than the intrinsic dimensionality of the manifold, the solution for <img class="formulaInl" alt="$w_i$" src="form_20.png"> is not unique. Because it can be desirable that <img class="formulaInl" alt="$k$" src="form_15.png"> is variable and because the intrinsic dimensionality is not necessarily known <em>a</em> <em>priori</em>, it is not straightforward to decide when the problem must be conditioned to provide a unique solution. Therefore, although this approach is an option in Dr. L., a simpler alternative is the default. In this case, <img class="formulaInl" alt="$w_i$" src="form_20.png"> is chosen to be the inverse Euclidean distance between the sample and the neighbor <img class="formulaInl" alt="$i$" src="form_21.png">. This approach allows for an arbitrarily high number of neighbors, however, will clearly fail in the case when a sample is outside the convex hull of its neighbors (due to the constraint that <img class="formulaInl" alt="$w_i$" src="form_20.png"> is positive).<p>
When forward or reverse mapping is performed using a method that does not obtain explicit maps (e.g. LLE and IsoMap), neighbor reconstruction mapping (NRM) will be performed. In this case, the user has the choice to use LLE weights or distance weights (as described above). Additionally, either k-nearest neighbors or epsilon neighbors can be used:<p>
<div class="fragment"><pre class="fragment">  <span class="comment">// Set the weights for reconstruction based on distances</span>
  <span class="comment">// -- Alternatively, 1 can be specified to use LLE least-squares weighting.</span>
  redmet-&gt;<a class="code" href="class_y_a_dim_reduce.html#a27">neighbor_weight_mode</a>(0);
  
  <span class="comment">// If we are using k-nearest neighbors, specify k</span>
  <span class="keywordtype">int</span> k=10;
  redmet-&gt;<a class="code" href="class_y_a_dim_reduce.html#a21">neighbors</a>(k);
  
  <span class="comment">// If we are using a ball, specify epsilon</span>
  <span class="keywordtype">double</span> eps=0.2;
  redmet-&gt;<a class="code" href="class_y_a_dim_reduce.html#a23">epsilon</a>(eps);
  
  <span class="comment">// Specify that we are using k-nearest neighbors</span>
  <span class="comment">// -- Alternatively, we could use 1 to specify epsilon neighbors</span>
  redmet-&gt;<a class="code" href="class_y_a_dim_reduce.html#a25">neighbor_mode</a>(0);

  <span class="comment">// Perform a forward map</span>
  redmet-&gt;<a class="code" href="class_y_a_dim_reduce.html#a6">forward_t</a>(input,output);
</pre></div><p>
For details, see <a class="el" href="class_y_a_dim_reduce.html">YADimReduce</a>.<p>
<br>
 <br>
 <h2><a class="anchor" name="pca">
Principal Component Analysis</a></h2>
PCA is a linear dimensionality reduction approach that has been widely applied to problems in almost every field of experimental science. The goal of PCA is to find a coordinate representation for data where the most variance is captured in the least number of coordinates. This representation can be found by performing an eigenvalue decomposition (or singular value decomposition) such that the resulting eigenvectors/singular vectors provide an orthonormal basis for the data while the eigenvalues/singular values provide information on the importance of each basis vector. Given the training set <img class="formulaInl" alt="$X$" src="form_12.png">, a row-centered matrix is calculated as <img class="formulaInl" alt="$\widetilde{X}_{D \times n} = (\widetilde{\mathbf{x}}_1, \dots, \widetilde{\mathbf{x}}_n)$" src="form_22.png">, where <img class="formulaInl" alt="$\widetilde{\mathbf{x}}_i = \mathbf{x}_i - \mathbf{m}$" src="form_23.png"> and <img class="formulaInl" alt="$\mathbf{m}_{D \times 1}$" src="form_24.png"> gives the row means. Eigen decomposition of the training set covariance matrix, <img class="formulaInl" alt="$\frac{1}{n} \widetilde{X} \widetilde{X}^T$" src="form_25.png">, is performed to give <img class="formulaInl" alt="$U P U^T$" src="form_26.png">. The forward map is then given by <img class="formulaInl" alt="$\Phi_{PCA}(\mathbf{x})=\widehat{U}^T (\mathbf{x}-\mathbf{m})$" src="form_27.png">, where <img class="formulaInl" alt="$\widehat{U}_{d \times n}$" src="form_28.png"> is the matrix composed of the first <img class="formulaInl" alt="$d$" src="form_29.png"> columns of <img class="formulaInl" alt="$U$" src="form_30.png"> corresponding to the eigenvectors with the largest eigenvalues. The reverse map is calculated as <img class="formulaInl" alt="$\phi_{PCA}(\mathbf{y})=\widehat{U} \mathbf{y} + \mathbf{m}.$" src="form_31.png"> The reconstruction error for PCA will be zero for <img class="formulaInl" alt="$d>=D-z$" src="form_32.png">, where <img class="formulaInl" alt="$z$" src="form_33.png"> is the number of non-zero eigenvalues in <img class="formulaInl" alt="$P$" src="form_34.png">. For details on the PCA routines, see <a class="el" href="class_y_a_p_c_a_reduce.html">YAPCAReduce</a>.<p>
<br>
 <br>
 <h2><a class="anchor" name="lle">
Locally Linear Embedding</a></h2>
Locally Linear Embedding (LLE) is a nonlinear dimensionality reduction method. LLE is performed by first solving for the location of each sample <img class="formulaInl" alt="$\mathbf{x}_i$" src="form_18.png"> in terms of its neighbors. For each sample, the neighbors are determined as all samples within a ball of specified radius centered on the sample or as the <img class="formulaInl" alt="$k$" src="form_15.png"> nearest neighbors. A weight matrix, <img class="formulaInl" alt="$W$" src="form_35.png">, is obtained by determining the weights in a linear combination of neighbors that best reconstruct each sample,<p>
{eqnarray} {min}_W &amp; E(W) =  \| {x}_i -  w_{ij} {x}_j \|^2  \ {subject to} &amp; \{ {array}{l} w_{ij} = 0 { if } {x}_i { not neighbor } {x}_j\  w_{ij} = 1 { for every } i, {array} . }<p>
where <img class="formulaInl" alt="$W = (w_{ij})$" src="form_36.png">. This problem has a closed form solution and assures not only that each approximation <img class="formulaInl" alt="$\mathbf{x}_i \approx \sum_j w_{ij} \mathbf{x}_j$" src="form_37.png"> lies in the subspace spanned by the <img class="formulaInl" alt="$k$" src="form_15.png"> neighbors of <img class="formulaInl" alt="$\mathbf{x}_i$" src="form_18.png">, but also that the solution <img class="formulaInl" alt="$W$" src="form_35.png"> is invariant to translation, rotation, and rescaling. These properties allow, by design, calculation of a linear mapping that is also invariant to translation, rotation, and rescaling. This mapping from the <img class="formulaInl" alt="$n$" src="form_8.png"> data samples <img class="formulaInl" alt="$\mathbf{x}_i$" src="form_18.png"> to the low dimensional embedding <img class="formulaInl" alt="$\mathbf{y}_i$" src="form_19.png"> is performed by minimizing the embedding cost function,<p>
<p class="formulaDsp">
<img class="formulaDsp" alt="\[ \Gamma=\displaystyle\sum_{i=1}^n\|\mathbf{y}_i-\sum_{j=1}^n w_{ij}\mathbf{y}_j\|^2. \]" src="form_38.png">
<p>
<p>
In this case, the weights <img class="formulaInl" alt="$w_{ij}$" src="form_39.png"> are fixed and the low-dimensional coordinates are optimized. This is a quadratic minimization problem with a unique global minimum. It can be solved as a sparse <img class="formulaInl" alt="$n \times n$" src="form_40.png"> eigen value problem where the bottom <img class="formulaInl" alt="$d$" src="form_29.png"> non-zero eigenvectors provide the embedding (the bottom eigenvalue is zero). From this, it can be seen that LLE assumes that a sample and its neighbors can be treated in a linear fashion. Global structure is maintained due to the overlap of neighbors in each local patch in the embedding cost function. A detailed description of LLE can be found in <a class="el" href="index.html#ref">[Roweis, 2000]</a> and <a class="el" href="index.html#ref">[Saul, 2003]</a>.<p>
Because the low-dimensional representation is optimized directly, no explicit maps are generated. Here, we use <img class="formulaInl" alt="$\Phi_{NRM}$" src="form_41.png"> and <img class="formulaInl" alt="$\phi_{NRM}$" src="form_42.png"> to perform mapping in terms of the initial LLE reduction as described above. LLE is parameterized by the neighboring method (kNN or epsilon) and any options for eigen decomposition. These should be set before performing a find operation:<p>
<div class="fragment"><pre class="fragment">  <span class="comment">// Use default eigen decomposition options</span>
  EigenOptions eigopts;

  <span class="comment">// If we are using k-nearest neighbors, specify k</span>
  <span class="keywordtype">int</span> k=10;
  redmet-&gt;<a class="code" href="class_y_a_dim_reduce.html#a21">neighbors</a>(k);
  
  <span class="comment">// If we are using a ball, specify epsilon</span>
  <span class="keywordtype">double</span> eps=0.2;
  redmet-&gt;<a class="code" href="class_y_a_dim_reduce.html#a23">epsilon</a>(eps);
  
  <span class="comment">// Specify that we are using k-nearest neighbors</span>
  <span class="comment">// -- Alternatively, we could use 1 to specify epsilon neighbors</span>
  redmet-&gt;<a class="code" href="class_y_a_dim_reduce.html#a25">neighbor_mode</a>(0);

  <span class="comment">// Set the dimensionality to reduce to</span>
  <span class="keywordtype">int</span> low_dim=2;
  
  <span class="comment">// Perform the reduction</span>
  redmet-&gt;<a class="code" href="class_y_a_dim_reduce.html#a5">find_t</a>(input, output, low_dim, eigopts);
</pre></div><p>
See <a class="el" href="class_y_a_l_l_e_reduce.html">YALLEReduce</a> for further details.<p>
<br>
 <br>
 <h2><a class="anchor" name="iso">
IsoMap</a></h2>
Isomap is a nonlinear dimensionality reduction algorithm, described in <a class="el" href="index.html#ref">[Tenenbaum, 2000]</a>. The first step in the Isomap algorithm is to impose a graph structure <img class="formulaInl" alt="$G(V,E,W)$" src="form_43.png"> on the input dataset <img class="formulaInl" alt="$X$" src="form_12.png">. Each sample <img class="formulaInl" alt="$\mathbf{x}_i\in X$" src="form_44.png"> is represented by a node <img class="formulaInl" alt="$v_i \in V$" src="form_45.png"> and two nodes <img class="formulaInl" alt="$v_i$" src="form_46.png"> and <img class="formulaInl" alt="$v_j$" src="form_47.png"> are connected by an edge <img class="formulaInl" alt="$(v_i, v_j) \in E$" src="form_48.png"> with weight <img class="formulaInl" alt="$w_{ij} \in W$" src="form_49.png"> if <img class="formulaInl" alt="$\mathbf{x}_i$" src="form_18.png"> is a a neighbor of <img class="formulaInl" alt="$\mathbf{x}_j$" src="form_50.png">. Neighbors are calculated in the same manner as perfromed in LLE. The weight of <img class="formulaInl" alt="$w_{ij}$" src="form_39.png"> is given by the Euclidean distance between <img class="formulaInl" alt="$\mathbf{x}_i$" src="form_18.png"> and <img class="formulaInl" alt="$\mathbf{x}_j$" src="form_50.png">. The second step in Isomap involves computation of the shortest paths between all nodes in <img class="formulaInl" alt="$G$" src="form_51.png">. These distances are stored pairwise in a matrix <img class="formulaInl" alt="$D_{G}$" src="form_52.png">. The distance matrix <img class="formulaInl" alt="$D_{G}$" src="form_52.png"> is intended to represent the distances between all samples on the manifold - the geodesic distances. Because these distances are Euclidean for each sample and its neighbors, Isomap makes the same assumption of local linearity as LLE. Unlike LLE, global distances between all neighbors are explicity calculated with the graph approximation to geodesic distances.<p>
Because all pairwaise distances are available, Multi-Dimensional Scaling (MDS) can be applied to <img class="formulaInl" alt="$D_{G}$" src="form_52.png"> to perform a low-dimensional embedding. MDS is a variant of PCA that starts with a distance matrix <img class="formulaInl" alt="$D_G$" src="form_53.png">, converts the distance matrix to an inner product matrix, and calculates the eigenvalue decomposition of the resulting matrix. For the case presented here, this is performed by squaring each element in the distance matrix <img class="formulaInl" alt="$D_{G}$" src="form_52.png">, double-centering the resulting matrix, and performing the eigenvalue decomposition to give <img class="formulaInl" alt="$U P U^T$" src="form_26.png">. The low-dimensional embedding is then given by <img class="formulaInl" alt="$Y=\widehat{U} \widehat{P}$" src="form_54.png">, where <img class="formulaInl" alt="$\widehat{U}_{d \times n}$" src="form_28.png"> is the matrix comprised by the first <img class="formulaInl" alt="$d$" src="form_29.png"> columns of <img class="formulaInl" alt="$U$" src="form_30.png"> corresponding to the eigenvectors with largest eigenvalues and <img class="formulaInl" alt="$\widehat{P}_{d \times d}$" src="form_55.png"> is the diagonal matrix containing the square roots of the largest <img class="formulaInl" alt="$d$" src="form_29.png"> eigenvalues.<p>
Like LLE, Isomap does not calculate explicit maps in order to perform an embedding. Here, we use <img class="formulaInl" alt="$\Phi_{NRM}$" src="form_41.png"> and <img class="formulaInl" alt="$\phi_{NRM}$" src="form_42.png"> to perform mapping in terms of the initial Isomap reduction as described above. Also like LLE, IsoMap is parameterized by the neighboring method (kNN or epsilon) and any options for eigen decomposition. These should be set before performing a find operation:<p>
<div class="fragment"><pre class="fragment">  <span class="comment">// Use default eigen decomposition options</span>
  EigenOptions eigopts;

  <span class="comment">// If we are using k-nearest neighbors, specify k</span>
  <span class="keywordtype">int</span> k=10;
  redmet-&gt;<a class="code" href="class_y_a_dim_reduce.html#a21">neighbors</a>(k);
  
  <span class="comment">// If we are using a ball, specify epsilon</span>
  <span class="keywordtype">double</span> eps=0.2;
  redmet-&gt;<a class="code" href="class_y_a_dim_reduce.html#a23">epsilon</a>(eps);
  
  <span class="comment">// Specify that we are using k-nearest neighbors</span>
  <span class="comment">// -- Alternatively, we could use 1 to specify epsilon neighbors</span>
  redmet-&gt;<a class="code" href="class_y_a_dim_reduce.html#a25">neighbor_mode</a>(0);

  <span class="comment">// Set the dimensionality to reduce to</span>
  <span class="keywordtype">int</span> low_dim=2;
  
  <span class="comment">// Perform the reduction</span>
  redmet-&gt;<a class="code" href="class_y_a_dim_reduce.html#a5">find_t</a>(input, output, low_dim, eigopts);
</pre></div><p>
IsoMap relies on a graph constructed from each point and its neighbors and there is no guarantee that a single connected graph will result from the neighboring procedures. In the case where multiple connected components are found, dimensionality reduction is performed separately on each component. Future mappings using NRM based on this dimensionality reduction will map points into their nearest component. Statistics on the number and sizes of connected components using different neighboring criteria can be obtained with the <a class="el" href="class_y_a_iso_reduce.html#a15">YAIsoReduce::component_stat()</a> routines.<p>
The runtime required for reduction with IsoMap can be reduced by using <em>landmark</em> points. In this case, a subset of points from the sampled data are chosen as landmark points that are used for computation of the distance matrix and for performing future mapping using NRM. In this case, a vector of indices for the landmarks is also supplied to the find routine (0 is first index). [For wrappers, the vector can be a row vector or a column vector.]<p>
<div class="fragment"><pre class="fragment">  <a class="code" href="class_y_a_iso_reduce.html">YAIsoReduce</a> redmet;
  
  ...
  
  redmet.findmap(input, output, low_dim, eigopts, landmarks);
</pre></div><p>
For details on these and other routines provided, see <a class="el" href="class_y_a_iso_reduce.html">YAIsoReduce</a>.<p>
<br>
 <br>
 <h2><a class="anchor" name="ate">
Autoencoder Neural Network</a></h2>
An <em>autoencoder</em> performs dimensionality reduction via a bottleneck architecture neural network. Autoencoders were originally introduced sometime in the early 1990s, but they have not been widely applied due to the extreme difficulty of the optimization problem associated with training the resulting network. However, a method was recently proposed for pre-training an autoencoder neural network using a Restricted Boltzmann Machine (RBM) in order to accelerate the optimization process <a class="el" href="index.html#ref">[Hinton, 2006]</a>. This method was used to obtain impressive results on a very large benchmark dataset of hand written digits.<p>
The autoencoder introduced in <a class="el" href="index.html#ref">[Hinton, 2006]</a> consists of weighted sums and compositions of the well-known function <img class="formulaInl" alt="$\sigma(x) = 1/(1+\exp(x))$" src="form_56.png">. These functions are separated into distinct layers, with interconnections between functions in adjacent layers defining the network structure. At each layer in the network, inputs into the next layer consist of terms of the form <img class="formulaInl" alt="$\sigma ( b_j + \sum_i v_i w_i )$" src="form_57.png">, where <img class="formulaInl" alt="$b_j$" src="form_58.png"> represents a bias, <img class="formulaInl" alt="$w_i$" src="form_20.png"> represents a weight, and <img class="formulaInl" alt="$v_i$" src="form_46.png"> represents an input from the previous network layer. The inputs to the first layer are taken to be the components of the original vectors in our dataset <img class="formulaInl" alt="$X = \{\mathbf{x}_1, \dots, \mathbf{x}_n \}$" src="form_59.png">. The weights and biases are then optimized such that the mean reconstruction error <img class="formulaInl" alt="$1/n \sum_i \|\mathbf{x}_i - \phi_{AE}(\Phi_{AE}(\mathbf{x}_i))\|$" src="form_60.png"> is minimized (where <img class="formulaInl" alt="$\Phi_{AE}$" src="form_61.png"> is the forward map and <img class="formulaInl" alt="$\phi_{AE}$" src="form_62.png"> is the reverse map given by the network).<p>
To provide an illustrative example, suppose we have a dataset <img class="formulaInl" alt="$X$" src="form_12.png"> with native dimension 784, for which we want to construct a 2-dimensional embedding. We first define a network structure such as 784--1000--500--250--2, where the integers in the sequence represent the number of <img class="formulaInl" alt="$\sigma$" src="form_63.png"> functions in each layer. When appropriately trained, this structure will perform a reduction of 784-dimensional data to a 2-dimensional embedding. The justification for the initial increase in dimension to 1000 is that because the <img class="formulaInl" alt="$\sigma$" src="form_63.png"> functions are inherently binary, we may experience a loss of information when going from normalized data in <img class="formulaInl" alt="$[0,1]$" src="form_64.png"> to values in <img class="formulaInl" alt="${0,1}$" src="form_65.png">; the possible loss of information resulting from this process is potentially counter-balanced by an initial increase in dimensionality. The encoding structure is then mirrored to form a 2--250--500--1000--784 decoding network structure. The encoder and decoder networks are then joined and training is performed on the aggregate network.<p>
As mentioned above, the optimization procedure for obtaining the autoencoder weights proceeds in two steps. In the first step, a RBM is trained. This training is performed for a user specified number of iterations. In the second step, the autoencoder weights are fine-tuned using back-propagation (BP). This step is also performed for a user specified number of iterations. In both cases a training set is used for the optimization and a test set is used to avoid overtraining. The training set is also split into batches to avoid overtraining, as well as to improve algorithm speed. During each iteration all of the batches are used in sequence.<p>
The layers of the neural network and corresponding weights yield an analytic expression for both the forward (<img class="formulaInl" alt="$\Phi_{AE}$" src="form_61.png">) and reverse (<img class="formulaInl" alt="$\phi_{AE}$" src="form_62.png">) maps that is optimized during training. This allows for future mapping of arbitrary points.<p>
When using the autoencoder, several additional commands must be executed before the find routine in order to parameterize the random number generator and the sizes for the layers. Additionally, many other parameters can be tuned for autoencoder training. The most important parameters are given in the example below.<p>
<div class="fragment"><pre class="fragment">  <span class="comment">// Set up a random number generator and seed it</span>
  MathRandom&lt;MathMersenneTwister&gt; rng;
  <span class="keywordtype">unsigned</span> <span class="keywordtype">long</span> seed=123456789;
  rng.seed(seed);
  
  <span class="comment">// Get a dimensionality reduction object</span>
  <a class="code" href="class_y_a_a_t_e_reduce.html">YAATEReduce</a> redmet;
  <span class="comment">// Low dimensionality</span>
  <span class="keywordtype">int</span> low_dim=4;
  
  <span class="comment">// Tell redmet which random number generator to use.</span>
  redmet.<a class="code" href="class_y_a_a_t_e_reduce.html#a4">set_rng</a>(&amp;rng);
  <span class="comment">// Give the layer sizes for the neural network</span>
  <span class="keywordtype">int</span> layers[4]={35 64 32 4};
  redmet.<a class="code" href="class_y_a_dim_reduce.html#a29">ae_layers</a>(YA_WRAP(layers,1,4));
  
  <span class="comment">// Fraction of the input matrix to be used for training</span>
  <span class="keywordtype">double</span> trac=0.8;
  redmet.<a class="code" href="class_y_a_a_t_e_reduce.html#a16">train_split</a>(tfrac);
  
  <span class="comment">// Iterations of RBM and back-propagation</span>
  redmet.<a class="code" href="class_y_a_a_t_e_reduce.html#a18">rbm_iters</a>(10);
  redmet.<a class="code" href="class_y_a_a_t_e_reduce.html#a20">bp_iters</a>(30);

  <span class="comment">// Number of samples in RBM batches and BP batches</span>
  redmet.<a class="code" href="class_y_a_a_t_e_reduce.html#a22">rbm_size</a>(10);
  redmet.<a class="code" href="class_y_a_a_t_e_reduce.html#a24">bp_size</a>(100);
  
  ...
</pre></div><p>
For additional routines, parameters, and test set specification, see <a class="el" href="class_y_a_a_t_e_reduce.html">YAATEReduce</a>.<p>
<br>
 <br>
 <h2><a class="anchor" name="intrinsic">
Intrinsic Dimensionality Estimation</a></h2>
Dimensionality reduction methods give an approach for obtaining a map <img class="formulaInl" alt="$\Phi(\mathbf{x}) : \mathbb{R}^D \rightarrow \mathbb{R}^d$" src="form_2.png">. In addition to having a method to calculate the maps, we must also determine an appropriate value for <img class="formulaInl" alt="$d$" src="form_29.png">. One obvious choice is to determine some metric for quantifying the success of dimensionality reduction and evaluate the reduction performance at different embedding dimensionalities. For PCA and MDS (IsoMap), this metric can be the residual variance. The eigenvalues obtained in these approaches give the variance in each dimension and therefore the sum of the <img class="formulaInl" alt="$d+1$" src="form_66.png"> to <img class="formulaInl" alt="$D$" src="form_67.png"> eigenvalues is a measure of the variance that is not accounted for in the reduction. When this value is near zero, little is gained from adding a dimension. Although LLE also solves an eigenproblem, the eigenvalues obtained have been shown to be unreliable in determining <img class="formulaInl" alt="$d$" src="form_29.png"> <a class="el" href="index.html#ref">[Saul, 2003]</a>.<p>
To obtain the eigenvalues from PCA, MDS, IsoMap, and LLE in Dr. L.:<p>
<div class="fragment"><pre class="fragment">  <span class="keywordtype">int</span> high_dim=20;
  <span class="keywordtype">int</span> low_dim=3;
  
  ...
  
  redmet.<a class="code" href="class_y_a_dim_reduce.html#a5">find_t</a>(input, output, low_dim, eigopts);
  
  <span class="keywordtype">double</span> eigen_values[high_dim];
  YA_WRAP(<span class="keywordtype">double</span>) eig_w(eigen_values,1,20);
  eig_w=redmet.eigenvalues();
</pre></div><p>
An alternative metric utilized in Isomap <a class="el" href="index.html#ref">[Tenenbaum, 2009]</a> is a geodesic distance correlation residual given by <img class="formulaInl" alt="$1-R^2(D_G,D_Y)$" src="form_68.png">, where <img class="formulaInl" alt="$R^2(D_G,D_Y)$" src="form_69.png"> is the correlation coefficient between geodesic distances <img class="formulaInl" alt="$D_G$" src="form_53.png"> and distances in the low-dimensional space <img class="formulaInl" alt="$D_Y$" src="form_70.png">. This metric requires knowledge of the geodesic distances, however. For linear subspaces, the geodesic distances are given by the Euclidean distances. Otherwise, a method for estimating the geodesic distances, such as the one provided in Isomap, must be utilized.<p>
To evaluate the distance residual using, for example, IsoMap:<p>
<div class="fragment"><pre class="fragment">  <span class="keywordtype">int</span> high_dim=20;
  <span class="keywordtype">int</span> low_dim=3;
  
  ...
  
  redmet.<a class="code" href="class_y_a_dim_reduce.html#a5">find_t</a>(input,output,low_dim,eigopts);
  
  <span class="comment">// Dimensionalities to calculate the distance statistics for:</span>
  <span class="keywordtype">int</span> dimensionalities[3]={15, 10, 5}; 
  <span class="comment">// Correlation coefficient between geodesic distance and low-euclidean</span>
  <span class="keywordtype">double</span> corrcoef[3];
  <span class="comment">// Coefficient of determination (R^2) between geodesic distances and low</span>
  <span class="keywordtype">double</span> rsquared[3];
  <span class="comment">// Wrappers</span>
  YA_WRAP(<span class="keywordtype">int</span>) wdim(dimensionalities,1,3);
  YA_WRAP(<span class="keywordtype">double</span>) wcorr(corrcoef,1,3);
  YA_WRAP(<span class="keywordtype">double</span>) wrsq(rsquared,1,3);
  <span class="comment">// Calculate the statistics and store in vectors</span>
  dist_residual(redmet.graph_dists(),output_matrix,dimensions,
                corrcoeff,rsquared);
                
  <span class="comment">// -- or if landmarks are used:</span>
  
  dist_residual(redmet.graph_dists(),output_matrix,dimensions,
                corrcoeff,rsquared,landmarks);  
</pre></div><p>
For PCA and MDS, the distance residual can be evaluated between the input and output matrices used in the find calculation. For LLE and the autoencoder, the distance residual is of little value.<p>
As discussed earlier, a more general method that allows comparison between different algorithms is the reconstruction error <a class="el" href="index.html#ref">[Hinton, 2006]</a>. To calculate the reconstruction error using existing maps stored in redmet:<p>
<div class="fragment"><pre class="fragment">  <span class="keywordtype">double</span> err=redmet.<a class="code" href="class_y_a_dim_reduce.html#a11">reconstruct_error</a>(input);
</pre></div><p>
Note that when using NRM for mapping (LLE, MDS, IsoMap), the reconstruction error for the samples used to obtain the maps will always be 0.<p>
The approaches listed above are often cited as methods for estimating the intrinsic dimensionality of a manifold. However, they all rely on dimensionality reduction methods that attempt an embedding of sample data in a space with lower dimensionality. Therefore, these approaches are really only suitable for estimating the smooth <em>embedding</em> <em>dimensionality</em>. This subtlety is important because the Whitney embedding theorem dictates that a smooth embedding of a <img class="formulaInl" alt="$d$" src="form_29.png">-manifold may require as many as <img class="formulaInl" alt="$2d+1$" src="form_71.png"> dimensions. Knowledge of the smooth embedding dimensionality is desirable for performing dimensionality reduction. For determining the <em>intrinsic</em> <em>dimensionality</em>, however, methods such as local-PCA might be more accurate for manifolds with complex structure. This is because they do not rely on a single-coordinate embedding of the entire manifold.<p>
Dr.L. provides a variant of local PCA, referred to as point PCA (see <a class="el" href="index.html#ref">[Brown, 2009]</a>) in order to estimate the intrinsic dimensionality. Taking the same approximations used in LLE and Isomap, we assume that a local region of a manifold given by a point and its nearest neighbors is approximately linear (local PCA differs from point PCA in that generalized clustering techniques such as vector quantization are used to determine locality). This assumption allows for estimation of intrinsic dimensionality by assessing the error in fitting each set of points to a lower-dimensional hyperplane. PCA can be utilized to perform this task; for a <img class="formulaInl" alt="$d$" src="form_29.png">-dimensional manifold, the residual variance should be near zero given an encoding with <img class="formulaInl" alt="$d$" src="form_29.png"> principal components. For example, in the case of a 2-dimensional manifold, the neighborhood of each point should reside on a 2-dimensional plane and therefore the variance in the data should be explained entirely by the first 2 principal components.<p>
For details on using point-PCA in Dr. L., see <a class="el" href="ya__dim__redfun_8h.html#a0">point_pca()</a> for k-nearest neighbors and <a class="el" href="ya__dim__redfun_8h.html#a2">point_pca_ep()</a> for epsilon neighbors.<p>
<br>
 <br>
 <h2><a class="anchor" name="em">
Example Manifolds</a></h2>
A variety of sample manifolds (all embedded in 3 dimensions), can be generated using Dr. L. as described in the file <a class="el" href="ya__manifold__samples_8h.html">ya_manifold_samples.h</a>. For example, to generate the japanese flag with 700 samples:<p>
<div class="fragment"><pre class="fragment"><span class="preprocessor">  #include "dimred/ya_manifold_samples.h"</span>

  <span class="keywordtype">int</span> nS=700;
  <span class="keywordtype">double</span> manifold[nS*3];
  <span class="keywordtype">int</span> colors[nS];
  YA_WRAP(<span class="keywordtype">double</span>) wmanifold(manifold,nS,3);
  
  <span class="comment">// Set up a random number generator and seed it</span>
  MathRandom&lt;MathMersenneTwister&gt; rng;
  <span class="keywordtype">unsigned</span> <span class="keywordtype">long</span> seed=123456789;
  rng.seed(seed);

  <span class="comment">// Fill the matrix with the samples</span>
  ya_manifold_japaneseflag(wmanifold,colors,nS,rng);
</pre></div><p>
<br>
 <br>
 <h2><a class="anchor" name="advanced">
Advanced Use of Library</a></h2>
For examples on more advanced use of the library, see the cml_drl executable supplied with Dr.L.<p>
<br>
 <br>
 <h2><a class="anchor" name="ref">
References</a></h2>
Brown, W.M., Martin, S., Pollock, S.N., Coutsias, E.A., Watson, J.-P. <b> Algorithmic Dimensionality Reduction for Molecular Structure Analysis.</b> <em>Journal of Chemical Physics</em>. 2009. 130: p. 044901.<p>
Hinton, G.E., Salakhutdinov, R.R. <b>Reducing the Dimensionality of Data with Neural Networks.</b> <em>Science</em>. 2006. 313: p.504-507.<p>
Roweis, S.T., Saul, L.K. <b> Nonlinear Dimensionality Reduction by Locally Linear Embedding.</b> <em>Science</em>. 2000. 290: p. 2323-2326.<p>
Saul, L. Roweis, S. <b>Think Globally, Fit Locally: Unsupervised Learning of Low Dimensional Manifolds</b>. <em>Journal of Machine Learning Research</em>. 2003. 4: p.119-155.<p>
Tenenbaum, J.B., de Silva, V., Langfor, J.C. <b>A Global Geometric Framework for Nonlinear Dimensionality Reduction.</b> <em>Science</em>. 2000. 290: p. 2319-2323.<p>
<br>
 <br>
 <br>
 <br>
 <hr size="1"><address style="align: right;"><small>Generated on Fri Jun 5 11:11:49 2009 for Dr.L. by&nbsp;
<a href="http://www.doxygen.org/index.html">
<img src="doxygen.png" alt="doxygen" align="middle" border="0"></a> 1.3.9.1 </small></address>
</body>
</html>
